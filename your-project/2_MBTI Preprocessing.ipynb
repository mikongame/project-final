{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HO-8OaBLuFxa"
   },
   "source": [
    "<img src=\"https://bit.ly/2VnXWr2\" width=\"100\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XuRHxVAnuFpc"
   },
   "source": [
    "# Final project: NLP to predict Myers-Briggs Personality Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4qu47z_6t3i7"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:24:12.835280Z",
     "start_time": "2020-05-19T20:24:12.827426Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8Ty3i6oejxgF"
   },
   "outputs": [],
   "source": [
    "# Data Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.plot\n",
    "from yellowbrick.text import UMAPVisualizer\n",
    "\n",
    "# Data Visualization for text\n",
    "from PIL import Image\n",
    "from os import path\n",
    "import os\n",
    "import random\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Text Processing\n",
    "import re\n",
    "import itertools\n",
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm\n",
    "from collections import Counter\n",
    "\n",
    "# Machine Learning packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import umap\n",
    "import sklearn.cluster as cluster\n",
    "# Ignore noise warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Export data\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "\n",
    "pd.set_option(\"display.max_column\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W8vaZS_-jxg0"
   },
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:13:54.014695Z",
     "start_time": "2020-05-19T20:13:53.386532Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "vLpZ8ocajxg0",
    "outputId": "5ec4d2bd-1044-4821-82f8-0dff32f298c3"
   },
   "outputs": [],
   "source": [
    "mbti_df = pd.read_csv(\"../your-project/data/mbti_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6RPgevlCw3pX"
   },
   "source": [
    "### Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:13:54.045507Z",
     "start_time": "2020-05-19T20:13:54.041352Z"
    }
   },
   "outputs": [],
   "source": [
    "type = [\"type\"]\n",
    "posts = [\"posts\"]\n",
    "columns = [*type, *posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:14:05.618474Z",
     "start_time": "2020-05-19T20:14:05.600456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti_df_raw = mbti_df\n",
    "mbti_df_raw[type] = mbti_df[type].fillna(\"\")\n",
    "mbti_df_raw[posts] = mbti_df[posts].fillna(\"\")\n",
    "mbti_df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare preprocessing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:14:08.225720Z",
     "start_time": "2020-05-19T20:14:08.217729Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_url(str_text_raw):\n",
    "    \"\"\"This function eliminate a string URL in a given text\"\"\"\n",
    "    str_text = re.sub(\"url_\\S+\", \"\", str_text_raw)\n",
    "    str_text = re.sub(\"email_\\S+\", \"\", str_text)\n",
    "    str_text = re.sub(\"phone_\\S+\", \"\", str_text)\n",
    "    return(re.sub(\"http[s]?://\\S+\", \"\", str_text))\n",
    "    \n",
    "def clean_punctuation(str_text_raw):\n",
    "    \"\"\"This function replace some of the troublemaker puntuation elements in a given text\"\"\"\n",
    "    return(re.sub(\"[$\\(\\)/|{|\\}#~\\[\\]^#;:!?¿]\", \" \", str_text_raw))\n",
    "\n",
    "def clean_unicode(str_text_raw):\n",
    "    \"\"\"This function eliminate non-unicode text\"\"\"\n",
    "    str_text = re.sub(\"&amp;\", \"\", str_text_raw)\n",
    "    return(re.sub(r\"[^\\x00-\\x7F]+\",\" \", str_text))\n",
    "                      \n",
    "def clean_dot_words(str_text_raw):\n",
    "    \"\"\"This function replace dots between words\"\"\"\n",
    "    return(re.sub(r\"(\\w+)\\.+(\\w+)\", r\"\\1 \\2\",str_text_raw))\n",
    "\n",
    "def clean_text(str_text_raw):\n",
    "    \"\"\"This function sets the text to lowercase and applies previous cleaning functions \"\"\"\n",
    "    str_text = str_text_raw.lower()\n",
    "    str_text = clean_dot_words(clean_punctuation(clean_unicode(clean_url(str_text))))\n",
    "    return(str_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization and lemmatization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:14:09.930829Z",
     "start_time": "2020-05-19T20:14:09.917890Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens_to_drop=[\"+\"]\n",
    "\n",
    "def string_to_token(string, str_pickle = None):\n",
    "    \"\"\"\n",
    "    This function takes a sentence and returns the list of tokens and all their information\n",
    "    * Text: The original text of the lemma.\n",
    "    * Lemma: Lemma.\n",
    "    * Orth: The hash value of the lemma.\n",
    "    * is alpha: Does the lemma consist of alphabetic characters?\n",
    "    * is digit: Does the lemma consist of digits?\n",
    "    * is_title: Is the token in titlecase? \n",
    "    * is_punct: Is the token punctuation?\n",
    "    * is_space: Does the token consist of whitespace characters?\n",
    "    * is_stop: Is the token part of a “stop list”?\n",
    "    * is_digit: Does the token consist of digits?\n",
    "    * lang: Language of the token\n",
    "    * tag: Fine-grained part-of-speech. The complete list is in: \n",
    "    https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html, also using: spacy.explain(\"RB\")\n",
    "    * pos: Coarse-grained part-of-speech.\n",
    "    * has_vector: A boolean value indicating whether a word vector is associated with the token.\n",
    "    * vector_norm: The L2 norm of the token’s vector representation.\n",
    "    * is_ovv: \"\"\"\n",
    "    doc = nlp(string)\n",
    "    l_token = [[token.text, token.lemma_, token.orth, token.is_alpha, token.is_digit, token.is_title, token.lang_, \n",
    "        token.tag_, token.pos_, token.has_vector, token.vector_norm, token.is_oov]\n",
    "        for token in doc if not token.is_punct | token.is_space | token.is_stop | token.is_digit | token.like_url \n",
    "               | token.like_num | token.like_email & token.is_oov]\n",
    "    pd_token = pd.DataFrame(l_token, columns=[\"text\", \"lemma\", \"orth\", \"is_alpha\", \"is_digit\", \"is_title\", \"language\",\n",
    "                                          \"tag\", \"part_of_speech\", \"has_vector\", \"vector_norm\", \"is_oov\"])\n",
    "    #drop problematic tokens\n",
    "    pd_token = pd_token[~pd_token[\"text\"].isin(tokens_to_drop)]\n",
    "    #Convert plural text to singular\n",
    "    pd_token[\"text_to_singular\"] = np.where(pd_token[\"tag\"].isin([\"NNPS\", \"NNS\"]), pd_token[\"lemma\"], pd_token[\"text\"])\n",
    "    if(str_pickle!=None):\n",
    "        pd_token.to_pickle(f\"data/output_pickles/{str_pickle}.pkl\") #Modified\n",
    "    del l_token\n",
    "    return(pd_token)\n",
    "\n",
    "def apply_cleaning(string):\n",
    "    \"\"\"\n",
    "    This function takes a sentence and returns a clean text\n",
    "    \"\"\"\n",
    "    doc = nlp(clean_text(string))\n",
    "    l_token = [token.text for token in doc if not token.is_punct | token.is_space | token.is_stop | \n",
    "               token.is_digit | token.like_url | token.like_num | token.like_email & token.is_oov]\n",
    "    return \" \".join(l_token)\n",
    "\n",
    "def apply_lemma(string):\n",
    "    \"\"\"\n",
    "    This function takes a sentence and returns a clean text\n",
    "    \"\"\"\n",
    "    doc = nlp(clean_text(string))\n",
    "    l_token = [token.lemma_ for token in doc if not token.is_punct | token.is_space | token.is_stop | \n",
    "               token.is_digit | token.like_url | token.like_num | token.like_email & token.is_oov]\n",
    "    return \" \".join(l_token)\n",
    "\n",
    "def list_to_bow(l_words):\n",
    "    \"\"\"\n",
    "    This function takes a list of words and create the bag of words ordered by desc order\n",
    "    \"\"\"\n",
    "    cv = CountVectorizer(l_words)\n",
    "    # show resulting vocabulary; the numbers are not counts, they are the position in the sparse vector.\n",
    "    count_vector=cv.fit_transform(l_words)\n",
    "    word_freq = Counter(l_words)\n",
    "    print(f\"Bag of words size: {count_vector.shape}\\nUnique words size: {len(word_freq)}\")\n",
    "    dict_word_freq = dict(word_freq.most_common())\n",
    "    return(dict_word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:14:22.987785Z",
     "start_time": "2020-05-19T20:14:13.578643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>infj</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entp</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intp</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intj</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entj</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  infj  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  entp  'I'm finding the lack of me in these posts ver...\n",
       "2  intp  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  intj  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  entj  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti_df_clean = pd.DataFrame(mbti_df_raw[[\"type\", \"posts\"]])\n",
    "for c in columns:\n",
    "    mbti_df_clean[c] = mbti_df_raw[c].apply(lambda row: clean_text(row))\n",
    "mbti_df_clean[\"posts\"] = mbti_df_raw[posts].apply(lambda x: \" \".join(x), axis=1)\n",
    "mbti_df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.nicepng.com/png/detail/148-1486992_discover-the-most-powerful-ways-to-automate-your.png\" width=\"1000\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.553968Z",
     "start_time": "2020-05-19T18:33:12.546961Z"
    }
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "This is a very consumming memory process, with average wall time: ~ 20 min. If you don't want to wait please go to the next step",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m This is a very consumming memory process, with average wall time: ~ 20 min. If you don't want to wait please go to the next step\n"
     ]
    }
   ],
   "source": [
    "raise SystemExit(\"This is a very consumming memory process, with average wall time: ~ 20 min. If you don't want to wait please go to the next step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:14:25.897307Z",
     "start_time": "2020-05-19T20:14:24.089133Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable = [\"ner\", \"parser\"]) \n",
    "nlp.max_length = 33000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:23.002237Z",
     "start_time": "2020-05-19T18:33:22.997250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8675, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti_df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:30.006108Z",
     "start_time": "2020-05-19T18:33:30.002118Z"
    }
   },
   "outputs": [],
   "source": [
    "mbti_df_clean_first = mbti_df_clean.iloc[:2169]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:32.217641Z",
     "start_time": "2020-05-19T18:33:32.213651Z"
    }
   },
   "outputs": [],
   "source": [
    "mbti_df_clean_second = mbti_df_clean[2169:4338]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:34.695263Z",
     "start_time": "2020-05-19T18:33:34.691299Z"
    }
   },
   "outputs": [],
   "source": [
    "mbti_df_clean_third = mbti_df_clean.iloc[4338:6507]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:37.430638Z",
     "start_time": "2020-05-19T18:33:37.426649Z"
    }
   },
   "outputs": [],
   "source": [
    "mbti_df_clean_fourth = mbti_df_clean.iloc[6507:8675]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End cleaning and tokenize rows using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-19T18:33:40.050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of type column: 10844\n",
      "Number of tokens created: 2169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for column in columns:    \n",
    "    str_bow_column_first = \" \".join(mbti_df_clean_first[column])\n",
    "    pd_token_first = string_to_token(str_bow_column_first, f\"token_first_{column}\")\n",
    "    print(f\"Length of {column} column: {len(str_bow_column_first)}\")\n",
    "    print(f\"Number of tokens created: {pd_token_first.shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.572892Z",
     "start_time": "2020-05-19T18:32:34.222Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for column in columns:    \n",
    "    str_bow_column_second = \" \".join(mbti_df_clean_second[column])\n",
    "    pd_token_second = string_to_token(str_bow_column_second, f\"token_second_{column}\")\n",
    "    print(f\"Length of {column} column: {len(str_bow_column_second)}\")\n",
    "    print(f\"Number of tokens created: {pd_token_second.shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.573888Z",
     "start_time": "2020-05-19T18:32:34.225Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for column in columns:    \n",
    "    str_bow_column_third = \" \".join(mbti_df_clean_third[column])\n",
    "    pd_token_third = string_to_token(str_bow_column_third, f\"token_third_{column}\")\n",
    "    print(f\"Length of {column} column: {len(str_bow_column_third)}\")\n",
    "    print(f\"Number of tokens created: {pd_token_third.shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.575883Z",
     "start_time": "2020-05-19T18:32:34.229Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for column in columns:    \n",
    "    str_bow_column_fourth = \" \".join(mbti_df_clean_fourth[column])\n",
    "    pd_token_fourth = string_to_token(str_bow_column_fourth, f\"token_fourth_{column}\")\n",
    "    print(f\"Length of {column} column: {len(str_bow_column_fourth)}\")\n",
    "    print(f\"Number of tokens created: {pd_token_fourth.shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the pickles into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:14:37.102763Z",
     "start_time": "2020-05-19T20:14:35.355646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading type info with 2169 rows\n",
      "Loading posts info with 1154000 rows\n",
      "Total rows loaded: 1156169\n",
      "Wall time: 1.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pd_token_first = pd.DataFrame(columns=[\"column\", \"text\", \"lemma\", \"orth\", \"is_alpha\", \"is_digit\", \"is_title\", \"language\", \"tag\", \n",
    "                                 \"part_of_speech\", \"has_vector\", \"vector_norm\", \"is_oov\", \"text_to_singular\"])\n",
    "for column in columns:\n",
    "    pd_temp = pd.read_pickle(f\"data/output_pickles/token_first_{column}.pkl\") #Modified\n",
    "    pd_temp[\"column\"] = column\n",
    "    print(f\"Loading {column} info with {pd_temp.shape[0]} rows\")\n",
    "    pd_token_first = pd.concat([pd_token_first, pd_temp])\n",
    "print(f\"Total rows loaded: {pd_token_first.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:14:41.350874Z",
     "start_time": "2020-05-19T20:14:39.685644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading type info with 2169 rows\n",
      "Loading posts info with 1152931 rows\n",
      "Total rows loaded: 1155100\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pd_token_second = pd.DataFrame(columns=[\"column\", \"text\", \"lemma\", \"orth\", \"is_alpha\", \"is_digit\", \"is_title\", \"language\", \"tag\", \n",
    "                                 \"part_of_speech\", \"has_vector\", \"vector_norm\", \"is_oov\", \"text_to_singular\"])\n",
    "for column in columns:\n",
    "    pd_temp = pd.read_pickle(f\"data/output_pickles/token_second_{column}.pkl\") #Modified\n",
    "    pd_temp[\"column\"] = column\n",
    "    print(f\"Loading {column} info with {pd_temp.shape[0]} rows\")\n",
    "    pd_token_second = pd.concat([pd_token_second, pd_temp])\n",
    "print(f\"Total rows loaded: {pd_token_second.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:14:43.585288Z",
     "start_time": "2020-05-19T20:14:41.932878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading type info with 2169 rows\n",
      "Loading posts info with 1152444 rows\n",
      "Total rows loaded: 1154613\n",
      "Wall time: 1.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pd_token_third = pd.DataFrame(columns=[\"column\", \"text\", \"lemma\", \"orth\", \"is_alpha\", \"is_digit\", \"is_title\", \"language\", \"tag\", \n",
    "                                 \"part_of_speech\", \"has_vector\", \"vector_norm\", \"is_oov\", \"text_to_singular\"])\n",
    "for column in columns:\n",
    "    pd_temp = pd.read_pickle(f\"data/output_pickles/token_third_{column}.pkl\") #Modified\n",
    "    pd_temp[\"column\"] = column\n",
    "    print(f\"Loading {column} info with {pd_temp.shape[0]} rows\")\n",
    "    pd_token_third = pd.concat([pd_token_third, pd_temp])\n",
    "print(f\"Total rows loaded: {pd_token_third.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:14:48.545352Z",
     "start_time": "2020-05-19T20:14:45.277728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading type info with 2168 rows\n",
      "Loading posts info with 1167724 rows\n",
      "Total rows loaded: 1169892\n",
      "Wall time: 3.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pd_token_fourth = pd.DataFrame(columns=[\"column\", \"text\", \"lemma\", \"orth\", \"is_alpha\", \"is_digit\", \"is_title\", \"language\", \"tag\", \n",
    "                                 \"part_of_speech\", \"has_vector\", \"vector_norm\", \"is_oov\", \"text_to_singular\"])\n",
    "for column in columns:\n",
    "    pd_temp = pd.read_pickle(f\"data/output_pickles/token_fourth_{column}.pkl\") #Modified\n",
    "    pd_temp[\"column\"] = column\n",
    "    print(f\"Loading {column} info with {pd_temp.shape[0]} rows\")\n",
    "    pd_token_fourth = pd.concat([pd_token_fourth, pd_temp])\n",
    "print(f\"Total rows loaded: {pd_token_fourth.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:14:50.030718Z",
     "start_time": "2020-05-19T20:14:50.013765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>orth</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>is_digit</th>\n",
       "      <th>is_title</th>\n",
       "      <th>language</th>\n",
       "      <th>tag</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>has_vector</th>\n",
       "      <th>vector_norm</th>\n",
       "      <th>is_oov</th>\n",
       "      <th>text_to_singular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type</td>\n",
       "      <td>infj</td>\n",
       "      <td>infj</td>\n",
       "      <td>11268318518583253733</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>22.320921</td>\n",
       "      <td>True</td>\n",
       "      <td>infj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type</td>\n",
       "      <td>entp</td>\n",
       "      <td>entp</td>\n",
       "      <td>9138091435026282108</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>19.919392</td>\n",
       "      <td>True</td>\n",
       "      <td>entp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type</td>\n",
       "      <td>intp</td>\n",
       "      <td>intp</td>\n",
       "      <td>10969288439530978247</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>19.970396</td>\n",
       "      <td>True</td>\n",
       "      <td>intp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type</td>\n",
       "      <td>intj</td>\n",
       "      <td>intj</td>\n",
       "      <td>421</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>True</td>\n",
       "      <td>18.208828</td>\n",
       "      <td>True</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type</td>\n",
       "      <td>entj</td>\n",
       "      <td>entj</td>\n",
       "      <td>13346206504721371118</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>True</td>\n",
       "      <td>18.694639</td>\n",
       "      <td>True</td>\n",
       "      <td>entj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  column  text lemma                  orth is_alpha is_digit is_title  \\\n",
       "0   type  infj  infj  11268318518583253733     True    False    False   \n",
       "1   type  entp  entp   9138091435026282108     True    False    False   \n",
       "2   type  intp  intp  10969288439530978247     True    False    False   \n",
       "3   type  intj  intj                   421     True    False    False   \n",
       "4   type  entj  entj  13346206504721371118     True    False    False   \n",
       "\n",
       "  language  tag part_of_speech has_vector  vector_norm is_oov text_to_singular  \n",
       "0       en  NNP          PROPN       True    22.320921   True             infj  \n",
       "1       en  NNP          PROPN       True    19.919392   True             entp  \n",
       "2       en  NNP          PROPN       True    19.970396   True             intp  \n",
       "3       en   NN           NOUN       True    18.208828   True             intj  \n",
       "4       en   NN           NOUN       True    18.694639   True             entj  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_token_first.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:14:51.134513Z",
     "start_time": "2020-05-19T20:14:51.116561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>orth</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>is_digit</th>\n",
       "      <th>is_title</th>\n",
       "      <th>language</th>\n",
       "      <th>tag</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>has_vector</th>\n",
       "      <th>vector_norm</th>\n",
       "      <th>is_oov</th>\n",
       "      <th>text_to_singular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1154338</th>\n",
       "      <td>posts</td>\n",
       "      <td>INFP</td>\n",
       "      <td>INFP</td>\n",
       "      <td>3649878657138915199</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>22.079365</td>\n",
       "      <td>True</td>\n",
       "      <td>INFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154339</th>\n",
       "      <td>posts</td>\n",
       "      <td>males</td>\n",
       "      <td>male</td>\n",
       "      <td>8448926857789959353</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>True</td>\n",
       "      <td>20.209568</td>\n",
       "      <td>True</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154340</th>\n",
       "      <td>posts</td>\n",
       "      <td>feel</td>\n",
       "      <td>feel</td>\n",
       "      <td>5741770584995928333</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>VBP</td>\n",
       "      <td>VERB</td>\n",
       "      <td>True</td>\n",
       "      <td>21.948505</td>\n",
       "      <td>True</td>\n",
       "      <td>feel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154341</th>\n",
       "      <td>posts</td>\n",
       "      <td>inadequate</td>\n",
       "      <td>inadequate</td>\n",
       "      <td>17995393726734896709</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>True</td>\n",
       "      <td>20.641933</td>\n",
       "      <td>True</td>\n",
       "      <td>inadequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154342</th>\n",
       "      <td>posts</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>17309040611306787464</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>True</td>\n",
       "      <td>19.141657</td>\n",
       "      <td>True</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        column        text       lemma                  orth is_alpha  \\\n",
       "1154338  posts        INFP        INFP   3649878657138915199     True   \n",
       "1154339  posts       males        male   8448926857789959353     True   \n",
       "1154340  posts        feel        feel   5741770584995928333     True   \n",
       "1154341  posts  inadequate  inadequate  17995393726734896709     True   \n",
       "1154342  posts        male        male  17309040611306787464     True   \n",
       "\n",
       "        is_digit is_title language  tag part_of_speech has_vector  \\\n",
       "1154338    False    False       en  NNP          PROPN       True   \n",
       "1154339    False    False       en  NNS           NOUN       True   \n",
       "1154340    False    False       en  VBP           VERB       True   \n",
       "1154341    False    False       en   JJ            ADJ       True   \n",
       "1154342    False    False       en   NN           NOUN       True   \n",
       "\n",
       "         vector_norm is_oov text_to_singular  \n",
       "1154338    22.079365   True             INFP  \n",
       "1154339    20.209568   True             male  \n",
       "1154340    21.948505   True             feel  \n",
       "1154341    20.641933   True       inadequate  \n",
       "1154342    19.141657   True             male  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_token_first.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add cleaned info to the dataset and store it into a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T19:40:53.684234Z",
     "start_time": "2020-05-19T19:40:52.982384Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-76abdf6b2e39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type_clean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mapply_cleaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts_clean'\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mapply_cleaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type_lemma'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mapply_lemma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts_lemma'\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mapply_lemma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-76abdf6b2e39>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type_clean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mapply_cleaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts_clean'\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mapply_cleaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type_lemma'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mapply_lemma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts_lemma'\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mapply_lemma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmbti_df_clean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-06b9505e918d>\u001b[0m in \u001b[0;36mapply_cleaning\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mtakes\u001b[0m \u001b[0ma\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mclean\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \"\"\"\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     l_token = [token.text for token in doc if not token.is_punct | token.is_space | token.is_stop | \n\u001b[0;32m     45\u001b[0m                token.is_digit | token.like_url | token.like_num | token.like_email & token.is_oov]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "mbti_df_clean['type_clean'] = mbti_df_clean['type'].apply(lambda x: apply_cleaning(x))\n",
    "mbti_df_clean['posts_clean']   = mbti_df_clean['posts'].apply(lambda x: apply_cleaning(x))\n",
    "mbti_df_clean['type_lemma'] = mbti_df_clean['type'].apply(lambda x: apply_lemma(x))\n",
    "mbti_df_clean['posts_lemma']   = mbti_df_clean['posts'].apply(lambda x: apply_lemma(x))\n",
    "mbti_df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.584859Z",
     "start_time": "2020-05-19T18:32:34.255Z"
    }
   },
   "outputs": [],
   "source": [
    "mbti_df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.585856Z",
     "start_time": "2020-05-19T18:32:34.257Z"
    }
   },
   "outputs": [],
   "source": [
    "mbti_df_clean.to_pickle('data/output_pickles/mbti_clean_text.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:15:16.519189Z",
     "start_time": "2020-05-19T20:15:16.368472Z"
    }
   },
   "outputs": [],
   "source": [
    "mbti_df_clean = pd.read_pickle(\"data/output_pickles/mbti_clean_text.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:15:20.379674Z",
     "start_time": "2020-05-19T20:15:20.371614Z"
    }
   },
   "outputs": [],
   "source": [
    "mbti_text = mbti_df[[\"type\",\"posts\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:15:22.321645Z",
     "start_time": "2020-05-19T20:15:21.052607Z"
    }
   },
   "outputs": [],
   "source": [
    "mbti_text = mbti_text.fillna(\"\")\n",
    "text_columns = mbti_text[[\"type\"]]\n",
    "text_columns[\"text\"] = mbti_text.iloc[:,1:].apply(lambda row: \" \".join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:15:23.630598Z",
     "start_time": "2020-05-19T20:15:23.622619Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:15:27.773659Z",
     "start_time": "2020-05-19T20:15:27.098612Z"
    }
   },
   "outputs": [],
   "source": [
    "text_columns = pd.DataFrame()\n",
    "text_columns[\"type\"] = mbti_df_clean[[\"type_lemma\"]].apply(lambda row: \" \".join(row.values.astype(str)), axis=1)\n",
    "text_columns[\"text\"] = mbti_df_clean[[\"posts_lemma\"]].apply(lambda row: \" \".join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:15:28.483586Z",
     "start_time": "2020-05-19T20:15:28.474947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>infj</td>\n",
       "      <td>intj moment sportscent play prank life change ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entp</td>\n",
       "      <td>find lack post alarming sex bore position exam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intp</td>\n",
       "      <td>good course know blessing curse absolutely pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intj</td>\n",
       "      <td>dear intp enjoy conversation day esoteric gabb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entj</td>\n",
       "      <td>fire silly misconception approach logically go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text\n",
       "0  infj  intj moment sportscent play prank life change ...\n",
       "1  entp  find lack post alarming sex bore position exam...\n",
       "2  intp  good course know blessing curse absolutely pos...\n",
       "3  intj  dear intp enjoy conversation day esoteric gabb...\n",
       "4  entj  fire silly misconception approach logically go..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_columns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.nicepng.com/png/detail/148-1486992_discover-the-most-powerful-ways-to-automate-your.png\" width=\"1000\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:15:31.701641Z",
     "start_time": "2020-05-19T20:15:31.696650Z"
    }
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Here it comes a very consumming memory process. You should better not start it till everything else has itereated propperly",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m Here it comes a very consumming memory process. You should better not start it till everything else has itereated propperly\n"
     ]
    }
   ],
   "source": [
    "raise SystemExit(\"Here it comes a very consumming memory process. You should better not start it till everything else has itereated propperly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:15:35.580610Z",
     "start_time": "2020-05-19T20:15:35.573629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_columns['text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:16:13.279981Z",
     "start_time": "2020-05-19T20:16:09.234718Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer().fit_transform(text_columns['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T19:43:16.690979Z",
     "start_time": "2020-05-19T19:43:16.686501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8675, 88023)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:25:40.025924Z",
     "start_time": "2020-05-19T20:25:37.724441Z"
    }
   },
   "outputs": [],
   "source": [
    "sparse.save_npz(\"data/output_pickles/tfidf.npz\", tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T19:43:20.757367Z",
     "start_time": "2020-05-19T19:43:20.373749Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:17:38.906862Z",
     "start_time": "2020-05-19T20:17:38.891865Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-0217e9da008a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfidf_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/output_csv/tfidf_df.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_df' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_df.to_csv(\"data/output_csv/tfidf_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T19:43:28.001386Z",
     "start_time": "2020-05-19T19:43:27.990425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 52925)\\t0.02457857898730888\\n  (0, 72839...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 78387)\\t0.045110794601750416\\n  (0, 2034...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 40085)\\t0.03322273941193883\\n  (0, 63157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 73593)\\t0.04268231701772938\\n  (0, 72871...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 63844)\\t0.0677461366096841\\n  (0, 10809)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0    (0, 52925)\\t0.02457857898730888\\n  (0, 72839...\n",
       "1    (0, 78387)\\t0.045110794601750416\\n  (0, 2034...\n",
       "2    (0, 40085)\\t0.03322273941193883\\n  (0, 63157...\n",
       "3    (0, 73593)\\t0.04268231701772938\\n  (0, 72871...\n",
       "4    (0, 63844)\\t0.0677461366096841\\n  (0, 10809)..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.601813Z",
     "start_time": "2020-05-19T18:32:34.301Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 100000    # max no. of words for tokenizer\n",
    "MAX_SEQUENCE_LENGTH = 200 # max length of each entry (sentence), including padding\n",
    "VALIDATION_SPLIT = 0.2   # data for validation (not used in training)\n",
    "EMBEDDING_DIM = 100      # embedding dimensions for word vectors (word2vec/GloVe)\n",
    "GLOVE_DIR = \"glove/glove.6B.\"+str(EMBEDDING_DIM)+\"d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.602811Z",
     "start_time": "2020-05-19T18:32:34.304Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(GLOVE_DIR)\n",
    "print('Loading GloVe from:', GLOVE_DIR,'...', end='')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    embeddings_index[word] = np.asarray(values[1:], dtype='float32')\n",
    "f.close()\n",
    "print(\"Done.\\n Proceeding with Embedding Matrix...\", end=\"\")\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(\" Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.603809Z",
     "start_time": "2020-05-19T18:32:34.307Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                           EMBEDDING_DIM,\n",
    "                           weights = [embedding_matrix],\n",
    "                           input_length = MAX_SEQUENCE_LENGTH,\n",
    "                           trainable=False,\n",
    "                           name = 'embeddings')\n",
    "embedded_sequences = embedding_layer(sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.604805Z",
     "start_time": "2020-05-19T18:32:34.311Z"
    }
   },
   "outputs": [],
   "source": [
    "x = LSTM(60, return_sequences=True,name='lstm_layer')(embedded_sequences)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "preds = Dense(6, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.606800Z",
     "start_time": "2020-05-19T18:32:34.316Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding = umap.UMAP(metric='hellinger', random_state=42).fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.607798Z",
     "start_time": "2020-05-19T18:32:34.318Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.608795Z",
     "start_time": "2020-05-19T18:32:34.320Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_df = pd.DataFrame(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.609792Z",
     "start_time": "2020-05-19T18:32:34.322Z"
    }
   },
   "outputs": [],
   "source": [
    "print(embedding_df[0].min())\n",
    "print(embedding_df[0].max())\n",
    "print(embedding_df[1].min())\n",
    "print(embedding_df[1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.610789Z",
     "start_time": "2020-05-19T18:32:34.325Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transformación de los embbeding a positivos\n",
    "embedding_positive = embedding-embedding_df[0].min()\n",
    "embedding_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.611791Z",
     "start_time": "2020-05-19T18:32:34.327Z"
    }
   },
   "outputs": [],
   "source": [
    "text_colums_umap = pd.DataFrame(embedding_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.612796Z",
     "start_time": "2020-05-19T18:32:34.330Z"
    }
   },
   "outputs": [],
   "source": [
    "text_colums_umap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.613799Z",
     "start_time": "2020-05-19T18:32:34.333Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "plt.scatter(embedding_positive[:, 0], embedding_positive[:, 1], s=0.5, cmap='Spectral')\n",
    "\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "plt.savefig(\"images/output_images/embedding_positive.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will get the original dataframe with its posts lengths and variances, dummies from every type and dummies on every type dimension axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.615777Z",
     "start_time": "2020-05-19T18:32:34.337Z"
    }
   },
   "outputs": [],
   "source": [
    "def var_row(row):\n",
    "    lst = []\n",
    "    for word in row.split(\"|||\"):\n",
    "        lst.append(len(word.split()))\n",
    "    return np.var(lst)\n",
    "\n",
    "mbti_df[\"words_per_comment\"] = mbti_df[\"posts\"].apply(lambda x: len(x.split())/50)\n",
    "mbti_df[\"variance_of_word_counts\"] = mbti_df[\"posts\"].apply(lambda x: var_row(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.616773Z",
     "start_time": "2020-05-19T18:32:34.339Z"
    }
   },
   "outputs": [],
   "source": [
    "type_dummies = pd.get_dummies(mbti_df[\"type\"])\n",
    "mbti_df.drop([\"posts\"], axis=1, inplace=True)\n",
    "mbti_df = pd.concat([mbti_df, type_dummies], axis=1,levels=None ,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.617771Z",
     "start_time": "2020-05-19T18:32:34.341Z"
    }
   },
   "outputs": [],
   "source": [
    "map1 = {\"I\": 0, \"E\": 1}\n",
    "map2 = {\"N\": 0, \"S\": 1}\n",
    "map3 = {\"T\": 0, \"F\": 1}\n",
    "map4 = {\"J\": 0, \"P\": 1}\n",
    "mbti_df[\"I-E\"] = mbti_df[\"type\"].astype(str).str[0]\n",
    "mbti_df[\"I-E\"] = mbti_df[\"I-E\"].map(map1)\n",
    "mbti_df[\"N-S\"] = mbti_df[\"type\"].astype(str).str[1]\n",
    "mbti_df[\"N-S\"] = mbti_df[\"N-S\"].map(map2)\n",
    "mbti_df[\"T-F\"] = mbti_df[\"type\"].astype(str).str[2]\n",
    "mbti_df[\"T-F\"] = mbti_df[\"T-F\"].map(map3)\n",
    "mbti_df[\"J-P\"] = mbti_df[\"type\"].astype(str).str[3]\n",
    "mbti_df[\"J-P\"] = mbti_df[\"J-P\"].map(map4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will merge umap results with the new numerical columns I created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.618768Z",
     "start_time": "2020-05-19T18:32:34.344Z"
    }
   },
   "outputs": [],
   "source": [
    "mbti_df.drop([\"type\"], axis=1, inplace=True)\n",
    "result = pd.concat([mbti_df, text_colums_umap], axis=1,levels=None ,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.619766Z",
     "start_time": "2020-05-19T18:32:34.346Z"
    }
   },
   "outputs": [],
   "source": [
    "result.to_csv(\"data/output_csv/result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.620769Z",
     "start_time": "2020-05-19T18:32:34.350Z"
    }
   },
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:33:12.622761Z",
     "start_time": "2020-05-19T18:32:34.352Z"
    }
   },
   "outputs": [],
   "source": [
    "mask = np.triu(np.ones_like(result.corr(), dtype=np.bool))\n",
    "plt.figure(figsize=(18,10))\n",
    "sns.heatmap(result.corr(), mask=mask, cmap='coolwarm', vmin=-1, vmax=1) \n",
    "sns.set_context(\"talk\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MBTI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
