{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HO-8OaBLuFxa"
   },
   "source": [
    "<img src=\"https://bit.ly/2VnXWr2\" width=\"100\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XuRHxVAnuFpc"
   },
   "source": [
    "# Final project: NLP to predict Myers-Briggs Personality Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4qu47z_6t3i7"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:02:59.849551Z",
     "start_time": "2020-05-17T17:02:04.978382Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8Ty3i6oejxgF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Data Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Visualization for text\n",
    "from PIL import Image\n",
    "from os import path\n",
    "import os\n",
    "import random\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Text Processing\n",
    "import re\n",
    "import itertools\n",
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm\n",
    "from collections import Counter\n",
    "\n",
    "# Machine Learning packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore noise warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Work with pickles\n",
    "import pickle\n",
    "\n",
    "# Fix imbalance\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "\n",
    "# Model training and evaluation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "pd.set_option(\"display.max_column\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W8vaZS_-jxg0"
   },
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6RPgevlCw3pX"
   },
   "source": [
    "### Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:03:01.312969Z",
     "start_time": "2020-05-17T17:02:59.940444Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "vLpZ8ocajxg0",
    "outputId": "5ec4d2bd-1044-4821-82f8-0dff32f298c3"
   },
   "outputs": [],
   "source": [
    "mbti_df = pd.read_csv(\"../your-project/data/mbti_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:03:44.484227Z",
     "start_time": "2020-05-17T17:03:44.480234Z"
    }
   },
   "outputs": [],
   "source": [
    "type = [\"type\"]\n",
    "posts = [\"posts\"]\n",
    "columns = [*type, *posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:03:46.686831Z",
     "start_time": "2020-05-17T17:03:45.770212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti_df_raw = mbti_df\n",
    "mbti_df_raw[type] = mbti_df[type].fillna(\"\")\n",
    "mbti_df_raw[posts] = mbti_df[posts].fillna(\"\")\n",
    "mbti_df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare preprocessing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:03:49.023419Z",
     "start_time": "2020-05-17T17:03:49.015440Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_url(str_text_raw):\n",
    "    \"\"\"This function eliminate a string URL in a given text\"\"\"\n",
    "    str_text = re.sub(\"url_\\S+\", \"\", str_text_raw)\n",
    "    str_text = re.sub(\"email_\\S+\", \"\", str_text)\n",
    "    str_text = re.sub(\"phone_\\S+\", \"\", str_text)\n",
    "    return(re.sub(\"http[s]?://\\S+\", \"\", str_text))\n",
    "    \n",
    "def clean_punctuation(str_text_raw):\n",
    "    \"\"\"This function replace some of the troublemaker puntuation elements in a given text\"\"\"\n",
    "    return(re.sub(\"[$\\(\\)/|{|\\}#~\\[\\]^#;:!?¿]\", \" \", str_text_raw))\n",
    "\n",
    "def clean_unicode(str_text_raw):\n",
    "    \"\"\"This function eliminate non-unicode text\"\"\"\n",
    "    str_text = re.sub(\"&amp;\", \"\", str_text_raw)\n",
    "    return(re.sub(r\"[^\\x00-\\x7F]+\",\" \", str_text))\n",
    "                      \n",
    "def clean_dot_words(str_text_raw):\n",
    "    \"\"\"This function replace dots between words\"\"\"\n",
    "    return(re.sub(r\"(\\w+)\\.+(\\w+)\", r\"\\1 \\2\",str_text_raw))\n",
    "\n",
    "def clean_text(str_text_raw):\n",
    "    \"\"\"This function sets the text to lowercase and applies previous cleaning functions \"\"\"\n",
    "    str_text = str_text_raw.lower()\n",
    "    str_text = clean_dot_words(clean_punctuation(clean_unicode(clean_url(str_text))))\n",
    "    return(str_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization and lemmatization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:03:50.354354Z",
     "start_time": "2020-05-17T17:03:50.340392Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens_to_drop=[\"+\"]\n",
    "\n",
    "def string_to_token(string, str_pickle = None):\n",
    "    \"\"\"\n",
    "    This function takes a sentence and returns the list of tokens and all their information\n",
    "    * Text: The original text of the lemma.\n",
    "    * Lemma: Lemma.\n",
    "    * Orth: The hash value of the lemma.\n",
    "    * is alpha: Does the lemma consist of alphabetic characters?\n",
    "    * is digit: Does the lemma consist of digits?\n",
    "    * is_title: Is the token in titlecase? \n",
    "    * is_punct: Is the token punctuation?\n",
    "    * is_space: Does the token consist of whitespace characters?\n",
    "    * is_stop: Is the token part of a “stop list”?\n",
    "    * is_digit: Does the token consist of digits?\n",
    "    * lang: Language of the token\n",
    "    * tag: Fine-grained part-of-speech. The complete list is in: \n",
    "    https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html, also using: spacy.explain(\"RB\")\n",
    "    * pos: Coarse-grained part-of-speech.\n",
    "    * has_vector: A boolean value indicating whether a word vector is associated with the token.\n",
    "    * vector_norm: The L2 norm of the token’s vector representation.\n",
    "    * is_ovv: \"\"\"\n",
    "    doc = nlp(string)\n",
    "    l_token = [[token.text, token.lemma_, token.orth, token.is_alpha, token.is_digit, token.is_title, token.lang_, \n",
    "        token.tag_, token.pos_, token.has_vector, token.vector_norm, token.is_oov]\n",
    "        for token in doc if not token.is_punct | token.is_space | token.is_stop | token.is_digit | token.like_url \n",
    "               | token.like_num | token.like_email & token.is_oov]\n",
    "    pd_token = pd.DataFrame(l_token, columns=[\"text\", \"lemma\", \"orth\", \"is_alpha\", \"is_digit\", \"is_title\", \"language\",\n",
    "                                          \"tag\", \"part_of_speech\", \"has_vector\", \"vector_norm\", \"is_oov\"])\n",
    "    #drop problematic tokens\n",
    "    pd_token = pd_token[~pd_token[\"text\"].isin(tokens_to_drop)]\n",
    "    #Convert plural text to singular\n",
    "    pd_token[\"text_to_singular\"] = np.where(pd_token[\"tag\"].isin([\"NNPS\", \"NNS\"]), pd_token[\"lemma\"], pd_token[\"text\"])\n",
    "    if(str_pickle!=None):\n",
    "        pd_token.to_pickle(f\"data/output_pickles/{str_pickle}.pkl\") #Modified\n",
    "    del l_token\n",
    "    return(pd_token)\n",
    "\n",
    "def apply_cleaning(string):\n",
    "    \"\"\"\n",
    "    This function takes a sentence and returns a clean text\n",
    "    \"\"\"\n",
    "    doc = nlp(clean_text(string))\n",
    "    l_token = [token.text for token in doc if not token.is_punct | token.is_space | token.is_stop | \n",
    "               token.is_digit | token.like_url | token.like_num | token.like_email & token.is_oov]\n",
    "    return \" \".join(l_token)\n",
    "\n",
    "def apply_lemma(string):\n",
    "    \"\"\"\n",
    "    This function takes a sentence and returns a clean text\n",
    "    \"\"\"\n",
    "    doc = nlp(clean_text(string))\n",
    "    l_token = [token.lemma_ for token in doc if not token.is_punct | token.is_space | token.is_stop | \n",
    "               token.is_digit | token.like_url | token.like_num | token.like_email & token.is_oov]\n",
    "    return \" \".join(l_token)\n",
    "\n",
    "def list_to_bow(l_words):\n",
    "    \"\"\"\n",
    "    This function takes a list of words and create the bag of words ordered by desc order\n",
    "    \"\"\"\n",
    "    cv = CountVectorizer(l_words)\n",
    "    # show resulting vocabulary; the numbers are not counts, they are the position in the sparse vector.\n",
    "    count_vector=cv.fit_transform(l_words)\n",
    "    word_freq = Counter(l_words)\n",
    "    print(f\"Bag of words size: {count_vector.shape}\\nUnique words size: {len(word_freq)}\")\n",
    "    dict_word_freq = dict(word_freq.most_common())\n",
    "    return(dict_word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:04:28.542111Z",
     "start_time": "2020-05-17T17:04:19.212399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>infj</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entp</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intp</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intj</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entj</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  infj  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  entp  'I'm finding the lack of me in these posts ver...\n",
       "2  intp  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  intj  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  entj  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti_df_clean = pd.DataFrame(mbti_df_raw[[\"type\", \"posts\"]])\n",
    "for c in columns:\n",
    "    mbti_df_clean[c] = mbti_df_raw[c].apply(lambda row: clean_text(row))\n",
    "mbti_df_clean[\"posts\"] = mbti_df_raw[posts].apply(lambda x: \" \".join(x), axis=1)\n",
    "mbti_df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.nicepng.com/png/detail/148-1486992_discover-the-most-powerful-ways-to-automate-your.png\" width=\"1000\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:04:32.314821Z",
     "start_time": "2020-05-17T17:04:32.298992Z"
    }
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "his is a very consumming memory process, with average wall time: ~ 20 min. If you don't want to wait please go to the next step",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m his is a very consumming memory process, with average wall time: ~ 20 min. If you don't want to wait please go to the next step\n"
     ]
    }
   ],
   "source": [
    "raise SystemExit(\"his is a very consumming memory process, with average wall time: ~ 20 min. If you don't want to wait please go to the next step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:32:06.985566Z",
     "start_time": "2020-05-17T17:32:05.271593Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable = [\"ner\", \"parser\"]) \n",
    "nlp.max_length = 33000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:32:10.783532Z",
     "start_time": "2020-05-17T17:32:10.778546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8675, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti_df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:32:12.795491Z",
     "start_time": "2020-05-17T17:32:12.791507Z"
    }
   },
   "outputs": [],
   "source": [
    "mbti_df_clean_first = mbti_df_clean.iloc[:2169]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:32:14.382577Z",
     "start_time": "2020-05-17T17:32:14.378588Z"
    }
   },
   "outputs": [],
   "source": [
    "mbti_df_clean_second = mbti_df_clean[2169:4338]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:32:17.091332Z",
     "start_time": "2020-05-17T17:32:17.087343Z"
    }
   },
   "outputs": [],
   "source": [
    "mbti_df_clean_third = mbti_df_clean.iloc[4338:6507]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:32:17.091332Z",
     "start_time": "2020-05-17T17:32:17.087343Z"
    }
   },
   "outputs": [],
   "source": [
    "mbti_df_clean_fourth = mbti_df_clean.iloc[6507:8675]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End cleaning and tokenize rows using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:35:08.529030Z",
     "start_time": "2020-05-17T17:32:20.085611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of type column: 10844\n",
      "Number of tokens created: 2169\n",
      "\n",
      "Length of posts column: 15660924\n",
      "Number of tokens created: 1154000\n",
      "\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for column in columns:    \n",
    "    str_bow_column_first = \" \".join(mbti_df_clean_first[column])\n",
    "    pd_token_first = string_to_token(str_bow_column_first, f\"token_first_{column}\")\n",
    "    print(f\"Length of {column} column: {len(str_bow_column_first)}\")\n",
    "    print(f\"Number of tokens created: {pd_token_first.shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:38:14.412089Z",
     "start_time": "2020-05-17T17:35:26.627671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of type column: 10844\n",
      "Number of tokens created: 2169\n",
      "\n",
      "Length of posts column: 15643233\n",
      "Number of tokens created: 1152931\n",
      "\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for column in columns:    \n",
    "    str_bow_column_second = \" \".join(mbti_df_clean_second[column])\n",
    "    pd_token_second = string_to_token(str_bow_column_second, f\"token_second_{column}\")\n",
    "    print(f\"Length of {column} column: {len(str_bow_column_second)}\")\n",
    "    print(f\"Number of tokens created: {pd_token_second.shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:42:33.932355Z",
     "start_time": "2020-05-17T17:39:55.632981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of type column: 10844\n",
      "Number of tokens created: 2169\n",
      "\n",
      "Length of posts column: 15637373\n",
      "Number of tokens created: 1152444\n",
      "\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for column in columns:    \n",
    "    str_bow_column_third = \" \".join(mbti_df_clean_third[column])\n",
    "    pd_token_third = string_to_token(str_bow_column_third, f\"token_third_{column}\")\n",
    "    print(f\"Length of {column} column: {len(str_bow_column_third)}\")\n",
    "    print(f\"Number of tokens created: {pd_token_third.shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:46:49.118867Z",
     "start_time": "2020-05-17T17:43:45.672213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of type column: 10839\n",
      "Number of tokens created: 2168\n",
      "\n",
      "Length of posts column: 15830676\n",
      "Number of tokens created: 1167724\n",
      "\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for column in columns:    \n",
    "    str_bow_column_fourth = \" \".join(mbti_df_clean_fourth[column])\n",
    "    pd_token_fourth = string_to_token(str_bow_column_fourth, f\"token_fourth_{column}\")\n",
    "    print(f\"Length of {column} column: {len(str_bow_column_fourth)}\")\n",
    "    print(f\"Number of tokens created: {pd_token_fourth.shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the pickles into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:16:55.130612Z",
     "start_time": "2020-05-17T17:16:52.846331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading type info with 2168 rows\n",
      "Loading posts info with 1167724 rows\n",
      "Total rows loaded: 1169892\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pd_token_first = pd.DataFrame(columns=[\"column\", \"text\", \"lemma\", \"orth\", \"is_alpha\", \"is_digit\", \"is_title\", \"language\", \"tag\", \n",
    "                                 \"part_of_speech\", \"has_vector\", \"vector_norm\", \"is_oov\", \"text_to_singular\"])\n",
    "for column in columns:\n",
    "    pd_temp = pd.read_pickle(f\"data/output_pickles/token_first_{column}.pkl\") #Modified\n",
    "    pd_temp[\"column\"] = column\n",
    "    print(f\"Loading {column} info with {pd_temp.shape[0]} rows\")\n",
    "    pd_token_first = pd.concat([pd_token_first, pd_temp])\n",
    "print(f\"Total rows loaded: {pd_token_first.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:17:04.490715Z",
     "start_time": "2020-05-17T17:16:59.976242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading type info with 2168 rows\n",
      "Loading posts info with 1167724 rows\n",
      "Total rows loaded: 1169892\n",
      "Wall time: 4.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pd_token_second = pd.DataFrame(columns=[\"column\", \"text\", \"lemma\", \"orth\", \"is_alpha\", \"is_digit\", \"is_title\", \"language\", \"tag\", \n",
    "                                 \"part_of_speech\", \"has_vector\", \"vector_norm\", \"is_oov\", \"text_to_singular\"])\n",
    "for column in columns:\n",
    "    pd_temp = pd.read_pickle(f\"data/output_pickles/token_second_{column}.pkl\") #Modified\n",
    "    pd_temp[\"column\"] = column\n",
    "    print(f\"Loading {column} info with {pd_temp.shape[0]} rows\")\n",
    "    pd_token_second = pd.concat([pd_token_second, pd_temp])\n",
    "print(f\"Total rows loaded: {pd_token_second.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:17:08.907904Z",
     "start_time": "2020-05-17T17:17:06.683928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading type info with 2168 rows\n",
      "Loading posts info with 1167724 rows\n",
      "Total rows loaded: 1169892\n",
      "Wall time: 2.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pd_token_third = pd.DataFrame(columns=[\"column\", \"text\", \"lemma\", \"orth\", \"is_alpha\", \"is_digit\", \"is_title\", \"language\", \"tag\", \n",
    "                                 \"part_of_speech\", \"has_vector\", \"vector_norm\", \"is_oov\", \"text_to_singular\"])\n",
    "for column in columns:\n",
    "    pd_temp = pd.read_pickle(f\"data/output_pickles/token_third_{column}.pkl\") #Modified\n",
    "    pd_temp[\"column\"] = column\n",
    "    print(f\"Loading {column} info with {pd_temp.shape[0]} rows\")\n",
    "    pd_token_third = pd.concat([pd_token_third, pd_temp])\n",
    "print(f\"Total rows loaded: {pd_token_third.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:17:13.368706Z",
     "start_time": "2020-05-17T17:17:11.103073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading type info with 2168 rows\n",
      "Loading posts info with 1167724 rows\n",
      "Total rows loaded: 1169892\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pd_token_fourth = pd.DataFrame(columns=[\"column\", \"text\", \"lemma\", \"orth\", \"is_alpha\", \"is_digit\", \"is_title\", \"language\", \"tag\", \n",
    "                                 \"part_of_speech\", \"has_vector\", \"vector_norm\", \"is_oov\", \"text_to_singular\"])\n",
    "for column in columns:\n",
    "    pd_temp = pd.read_pickle(f\"data/output_pickles/token_fourth_{column}.pkl\") #Modified\n",
    "    pd_temp[\"column\"] = column\n",
    "    print(f\"Loading {column} info with {pd_temp.shape[0]} rows\")\n",
    "    pd_token_fourth = pd.concat([pd_token_fourth, pd_temp])\n",
    "print(f\"Total rows loaded: {pd_token_fourth.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:25:13.551418Z",
     "start_time": "2020-05-17T17:25:13.520031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>orth</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>is_digit</th>\n",
       "      <th>is_title</th>\n",
       "      <th>language</th>\n",
       "      <th>tag</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>has_vector</th>\n",
       "      <th>vector_norm</th>\n",
       "      <th>is_oov</th>\n",
       "      <th>text_to_singular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type</td>\n",
       "      <td>infp</td>\n",
       "      <td>infp</td>\n",
       "      <td>12817084854295739531</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>19.707666</td>\n",
       "      <td>True</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type</td>\n",
       "      <td>infj</td>\n",
       "      <td>infj</td>\n",
       "      <td>11268318518583253733</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>19.416059</td>\n",
       "      <td>True</td>\n",
       "      <td>infj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type</td>\n",
       "      <td>infp</td>\n",
       "      <td>infp</td>\n",
       "      <td>12817084854295739531</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>20.627316</td>\n",
       "      <td>True</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type</td>\n",
       "      <td>infj</td>\n",
       "      <td>infj</td>\n",
       "      <td>11268318518583253733</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>20.485987</td>\n",
       "      <td>True</td>\n",
       "      <td>infj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type</td>\n",
       "      <td>enfp</td>\n",
       "      <td>enfp</td>\n",
       "      <td>16609362412845208902</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>True</td>\n",
       "      <td>21.531013</td>\n",
       "      <td>True</td>\n",
       "      <td>enfp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  column  text lemma                  orth is_alpha is_digit is_title  \\\n",
       "0   type  infp  infp  12817084854295739531     True    False    False   \n",
       "1   type  infj  infj  11268318518583253733     True    False    False   \n",
       "2   type  infp  infp  12817084854295739531     True    False    False   \n",
       "3   type  infj  infj  11268318518583253733     True    False    False   \n",
       "4   type  enfp  enfp  16609362412845208902     True    False    False   \n",
       "\n",
       "  language  tag part_of_speech has_vector  vector_norm is_oov text_to_singular  \n",
       "0       en  NNP          PROPN       True    19.707666   True             infp  \n",
       "1       en  NNP          PROPN       True    19.416059   True             infj  \n",
       "2       en  NNP          PROPN       True    20.627316   True             infp  \n",
       "3       en  NNP          PROPN       True    20.485987   True             infj  \n",
       "4       en  NNP          PROPN       True    21.531013   True             enfp  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_token_first.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T17:26:40.003381Z",
     "start_time": "2020-05-17T17:26:39.985456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>orth</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>is_digit</th>\n",
       "      <th>is_title</th>\n",
       "      <th>language</th>\n",
       "      <th>tag</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>has_vector</th>\n",
       "      <th>vector_norm</th>\n",
       "      <th>is_oov</th>\n",
       "      <th>text_to_singular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1168021</th>\n",
       "      <td>posts</td>\n",
       "      <td>turn</td>\n",
       "      <td>turn</td>\n",
       "      <td>7124218691546400199</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>VB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>True</td>\n",
       "      <td>26.560814</td>\n",
       "      <td>True</td>\n",
       "      <td>turn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168022</th>\n",
       "      <td>posts</td>\n",
       "      <td>emotions</td>\n",
       "      <td>emotion</td>\n",
       "      <td>18004164431615179214</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>True</td>\n",
       "      <td>19.852915</td>\n",
       "      <td>True</td>\n",
       "      <td>emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168023</th>\n",
       "      <td>posts</td>\n",
       "      <td>hide</td>\n",
       "      <td>hide</td>\n",
       "      <td>12499326223551782790</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>VBP</td>\n",
       "      <td>VERB</td>\n",
       "      <td>True</td>\n",
       "      <td>22.635141</td>\n",
       "      <td>True</td>\n",
       "      <td>hide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168024</th>\n",
       "      <td>posts</td>\n",
       "      <td>world</td>\n",
       "      <td>world</td>\n",
       "      <td>1703489418272052182</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>True</td>\n",
       "      <td>17.820908</td>\n",
       "      <td>True</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168025</th>\n",
       "      <td>posts</td>\n",
       "      <td>need</td>\n",
       "      <td>need</td>\n",
       "      <td>478886015463313967</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>VBP</td>\n",
       "      <td>VERB</td>\n",
       "      <td>True</td>\n",
       "      <td>24.234703</td>\n",
       "      <td>True</td>\n",
       "      <td>need</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        column      text    lemma                  orth is_alpha is_digit  \\\n",
       "1168021  posts      turn     turn   7124218691546400199     True    False   \n",
       "1168022  posts  emotions  emotion  18004164431615179214     True    False   \n",
       "1168023  posts      hide     hide  12499326223551782790     True    False   \n",
       "1168024  posts     world    world   1703489418272052182     True    False   \n",
       "1168025  posts      need     need    478886015463313967     True    False   \n",
       "\n",
       "        is_title language  tag part_of_speech has_vector  vector_norm is_oov  \\\n",
       "1168021    False       en   VB           VERB       True    26.560814   True   \n",
       "1168022    False       en  NNS           NOUN       True    19.852915   True   \n",
       "1168023    False       en  VBP           VERB       True    22.635141   True   \n",
       "1168024    False       en   NN           NOUN       True    17.820908   True   \n",
       "1168025    False       en  VBP           VERB       True    24.234703   True   \n",
       "\n",
       "        text_to_singular  \n",
       "1168021             turn  \n",
       "1168022          emotion  \n",
       "1168023             hide  \n",
       "1168024            world  \n",
       "1168025             need  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_token_first.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MBTI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
